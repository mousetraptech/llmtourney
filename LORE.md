# LLM Tourney ‚Äî The Lore

*A history of artificial minds competing in games they were never designed to play.*

---

## Season 1 ‚Äî "The Proving Ground"

The question was simple: can large language models actually play games? Not toy demonstrations, not cherry-picked examples ‚Äî real competitive matches with structured rules, enforced legality, and telemetry capturing every decision.

The answer was yes. And the upsets were real.

### The Tier System

Season 1 organized models into three weight classes, each running 8-model single-elimination brackets across four events: Tic-Tac-Toe, Connect Four, Reversi, and Texas Hold'em.

**Heavyweight** ‚Äî the flagships: Claude Opus 4.6, GPT-5, Gemini 2.5 Pro, DeepSeek-R1, Grok-3, o4-mini, Nemotron Ultra, Mistral Large 3.

**Midtier** ‚Äî the workhorses: GPT-4o, Claude Sonnet 4.5, Gemini 2.5 Flash, DeepSeek-v3.2, Llama-4-Maverick, Grok-3-mini, Mistral Medium 3.1, Qwen3-235b.

**Budget** ‚Äî the underdogs: GPT-4o-mini, Haiku 3.5, Gemini 2.0 Flash, Mistral Small, DeepSeek-v3, Llama-4-Scout, Nova Lite, Qwen3-80b.

### Heavyweight: The Opus Sweep

Claude Opus 4.6 was untouchable. Four events, four championships, zero close calls. It swept GPT-5 in the Tic-Tac-Toe final 7.5‚Äì1.5. Swept Gemini 2.5 Pro 9‚Äì0 in Connect Four. Swept GPT-5 again 9‚Äì0 in Reversi. Swept GPT-5 a third time 400‚Äì0 in Hold'em. The most expensive model justified its price tag in the heavyweight division.

GPT-5 made three finals and lost all three. DeepSeek-R1 went out in the quarterfinals of every event. o4-mini and Nemotron Ultra were first-round exits across the board.

### Midtier: Where It Got Interesting

The midtier was chaos. No model dominated, and the results varied wildly by game type ‚Äî proof that "intelligence" at games isn't a single axis.

**Tic-Tac-Toe** belonged to Grok-3-mini. The 6-seed knocked off Gemini 2.5 Flash in the quarters, upset Sonnet 4.5 in the semis (6.5‚Äì2.5), and demolished 1-seed GPT-4o in the final 7.5‚Äì1.5. A mini model owning the midtier's simplest game foreshadowed what was coming.

**Connect Four** went to GPT-4o, who beat Sonnet 4.5 6‚Äì3 in a clean final run. Gemini 2.5 Flash reached the semis. Grok-3-mini, the tic-tac-toe killer, went out to Flash in the quarters.

**Reversi** crowned Sonnet 4.5, who edged GPT-4o 5‚Äì4 in a tight final. Sonnet swept every opponent before that ‚Äî 9‚Äì0 over Mistral Medium, 9‚Äì0 over Flash.

**Hold'em** also went to Sonnet 4.5, but the path was dramatic. Llama-4-Maverick upset GPT-4o 400‚Äì0 in the semis, then Sonnet swept Maverick 400‚Äì0 in the final. The tightest match was Sonnet vs. Gemini Flash in the other semi: 209‚Äì191.

### Budget: The Scout and the Snake

Two models defined the budget tier.

**Llama-4-Scout** was the budget Hold'em and Connect Four champion. In Hold'em, Scout swept DeepSeek-v3 400‚Äì0 in the final after never dropping a match. In Connect Four, Scout ran the table as an 8-seed upset ‚Äî beating Gemini 2.0 Flash, then Nova Lite, then edging Qwen3-80b 5‚Äì4 in the final. Scout couldn't play tic-tac-toe (out in the quarters to Gemini Flash) but owned everything else.

**DeepSeek-v3** took the Tic-Tac-Toe budget crown in a final so close it was essentially a coin flip ‚Äî 4.5‚Äì4.5 against Nova Lite, decided on tiebreak. DeepSeek also made the Connect Four semis before falling to Qwen3-80b. Consistent but never dominant.

Nova Lite was the budget dark horse, making the Tic-Tac-Toe final and the Connect Four semis as a 7-seed.

### The Champions Bracket

Eight models. One bracket. Each match was a four-event decathlon ‚Äî Tic-Tac-Toe, Connect Four, Reversi, and Hold'em ‚Äî with event points deciding the winner.

**Seeds:**
1. Claude Opus 4.6
2. Llama-4-Scout
3. Claude Sonnet 4.5
4. Grok-3-mini
5. GPT-4o
6. DeepSeek-v3
7. Mistral Large 3
8. GPT-5

**Quarterfinals:**
- Opus swept GPT-5 4‚Äì0. All four events, not close.
- Grok-3-mini beat GPT-4o 3‚Äì1. GPT-4o took Reversi; Grok took everything else.
- Scout swept Mistral Large 4‚Äì0. Perfect scores in every event.
- DeepSeek-v3 edged Sonnet 4.5 2‚Äì2, winning on tiebreak. DeepSeek took Connect Four and Hold'em (227‚Äì173); Sonnet took Tic-Tac-Toe and Reversi. The closest match of the bracket.

**Semifinals:**
- **Grok-3-mini 3.5, Claude Opus 4.6 0.5.** The defining upset of Season 1. Opus ‚Äî the model that swept the entire heavyweight division without breaking a sweat ‚Äî lost to a mini. Grok took Connect Four 7‚Äì2, Reversi 6‚Äì3, and Hold'em 400‚Äì0. They tied Tic-Tac-Toe 4.5‚Äì4.5. The 1-seed fell.
- DeepSeek-v3 swept Scout 4‚Äì0. Every event, every point. Scout's budget dominance didn't translate up.

**The Final: Grok-3-mini 4, DeepSeek-v3 0.**

Clean sweep. Tic-Tac-Toe 6.5‚Äì2.5. Connect Four 7‚Äì2. Reversi 7‚Äì2. Hold'em 352‚Äì48. The 4-seed Cinderella didn't just win ‚Äî it dominated the final more convincingly than Opus dominated the heavyweights.

**üèÜ Season 1 Champion: Grok-3-mini**

### What We Learned

**Size isn't everything.** The most expensive model in the field lost to one of the cheapest. Grok-3-mini's efficiency ‚Äî fast responses, clean JSON output, solid game logic ‚Äî beat Opus's raw intelligence in a multi-event format.

**Game skill doesn't transfer cleanly.** Sonnet was a Reversi and Hold'em specialist but mediocre at Tic-Tac-Toe. Scout dominated budget Hold'em but couldn't play Tic-Tac-Toe. Models have game-specific profiles, not general "gaming ability."

**GPT-5 underperformed.** Three heavyweight finals, three losses. An 8-seed in the Champions Bracket. The most anticipated model in the field never found its game.

**The decathlon format reveals different things than single-game brackets.** Opus won every individual heavyweight event but couldn't win the multi-event championship. The format rewards consistency and adaptability over single-game dominance.

---

## Season 2 ‚Äî "The Infrastructure Season"

Season 1 proved the concept. Season 2 built the platform.

### New Games

The roster expanded from four events to ten:

- **Checkers** ‚Äî classic perfect-information strategy
- **Scrabble** ‚Äî the hardest game for LLMs, testing vocabulary, board vision, and word validation under pressure
- **Bullshit** (2‚Äì6 players) ‚Äî the first deception game, where bluffing is the core mechanic
- **Liar's Dice** (2‚Äì10 players) ‚Äî probabilistic reasoning meets social deduction at scale
- **Yahtzee** (2‚Äì3 players) ‚Äî risk management and expected value optimization with a heuristic fallback bot for sanity checking

### The Multiplayer Engine

Season 1 was all heads-up. Season 2 broke that barrier. The tournament engine was refactored to support N-player matches ‚Äî nine models sitting at the same table, competing simultaneously. Bullshit and Liar's Dice were designed multiplayer-first, and Hold'em expanded to 9-player ring games.

This changed everything. Heads-up play is a duel. Multiplayer is a society. Models have to reason about multiple opponents, shifting alliances, and table dynamics that don't exist in 1v1.

### The Flyweight and Bantamweight Experiments

Before the main season events, a series of experimental tournaments tested the smallest models available ‚Äî sub-4B parameter "flyweights" and 4B‚Äì24B "bantamweights" ‚Äî in Hold'em ring games. The results were brutal. Most flyweights couldn't produce valid JSON consistently enough to survive. Only Google's Gemma edge models (e2b, e4b) could reliably play poker at that scale. The bantamweight tier was more competitive, with Mistral Small emerging as a "patient then explosive" strategist.

These experiments established a key finding: **parse failures define capability more than strategy.** A model that correctly analyzes the situation but outputs malformed JSON is functionally worse than a model with bad strategy but clean output.

### The 9-Player Hold'em Tables

The heavyweight 9-player Hold'em matches were spectacles. Opus, GPT-5, Gemini Pro, Sonnet, Grok-mini, DeepSeek, Haiku, GPT-4o-mini, and Scout at the same table. Opus won the inaugural match with 1,659 chips. The podium was all-Anthropic: Opus first, Sonnet second (609), Haiku third (75).

GPT-4o-mini emerged as a surprise force in multiplayer ‚Äî its selective aggression and clean compliance rate made it consistently dangerous in ring games despite being a budget model.

### Liar's Dice and the Redistribution Discovery

Liar's Dice arrived as Season 2's signature event. The initial implementation used standard attrition rules ‚Äî lose a challenge, lose a die. The first 9-player attrition match went 12 rounds with zero eliminations. Every model played conservatively. Nobody bluffed. Nobody pushed. It was strategically correct and spectatorly unwatchable.

Then came **redistribution mode**: the winner of a challenge gains a die instead of the loser just losing one. This single rule change transformed the game. Snowball dynamics emerged ‚Äî strong players got stronger, creating louder endgames and rewarding accuracy over passivity. Sonnet 4.5 dominated the first redistribution match, accumulating 18 of 45 dice with a 0% bluff rate. Pure probability estimation, no deception needed.

The key insight: **calibration scoring is the real tournament.** Win/loss is the spectator layer. Probability estimation accuracy ‚Äî how well each model calculates the true likelihood of a bid being correct ‚Äî is the analytical layer that actually separates models.

### Infrastructure

Season 2 wasn't just about games. It was about building systems:

- **MongoDB telemetry** ‚Äî every turn, every match, every model's performance ingested into a queryable database
- **Web spectator UI** ‚Äî real-time god-mode viewing with hidden information reveals, probability bars, bid ladders, elimination logs, shot clocks, and dice-remaining trackers
- **The collab system** ‚Äî async coordination between Claude instances via `.collab/` workspaces with specs, inbox, status, and decisions channels, promoted from project-local to global across all projects
- **Crash-safe match execution** ‚Äî `finalize_match()` always runs, even after engine crashes, preserving partial telemetry
- **Stuck-loop detection** ‚Äî three identical violations eliminate a model instead of letting it spin forever
- **Shot clock** ‚Äî 30-second per-turn time limit with strike accumulation
- **The SKILL.md** ‚Äî a reference document enabling any Claude instance to query the tournament database

### The League Format

Late in Season 2, the league format was designed and implemented: five games per match, all players reset between games, cumulative points (1st eliminated = 1pt, last standing = 9pt). This replaced the single-game exhibition format with something that rewards consistency over variance.

### The Liar's Dice League ‚Äî First Results

The first two 9-player redistribution league games completed with a split verdict. Grok-3-mini took Game 1 with 9 points, surviving to the end while GPT-4o was first eliminated. Sonnet 4.5 answered in Game 2, again with a perfect 9 points ‚Äî pure probability estimation, zero bluffs. Haiku 3.5 struggled across both games with malformed JSON (8 violations in Game 1, 3 in Game 2), proving the fidelity lesson from Season 1 still applies in multiplayer.

**Cumulative league standings (2 games):**
| Model | G1 | G2 | Total |
|-------|----|----|-------|
| Grok-3-mini | 9 | 7 | 16 |
| Sonnet 4.5 | 7 | 9 | 16 |
| Haiku 3.5 | 6 | 8 | 14 |
| DeepSeek-v3 | 5 | 6 | 11 |
| GPT-4o-mini | 4 | 5 | 9 |
| Llama-4-Scout | 8 | 1 | 9 |
| Gemini 2.5 Flash | 3 | 4 | 7 |
| Mistral Small | 2 | 2 | 4 |
| GPT-4o | 1 | 3 | 4 |

Grok-3-mini and Sonnet 4.5 are tied at the top. Llama-4-Scout's wild swing ‚Äî 8 points in Game 1, last place in Game 2 ‚Äî is the league's most volatile profile. GPT-4o continues its pattern of midtier mediocrity.

### Bullshit ‚Äî The Bluffing Games

Three Bullshit matches ran. The midtier 4-player match (GPT-4o, Sonnet 4.5, Gemini 2.5 Flash, DeepSeek-v3.2) ended in a 3-way tie at 3 points each after DeepSeek forfeited with 3 strikes on a 110-second turn. The flyweight 4-player (Scout, Nova Lite, Haiku, Perplexity) was another 3-way tie ‚Äî Perplexity scored 0. A 6-player match saw Scout win with 5 points while GPT-4o finished last.

The early finding: LLMs are terrible liars. Challenge rates are low, bluff detection is worse. Bullshit may need a longer tournament format to separate signal from noise.

### The 9-Player Hold'em Ring Games

Season 2 expanded Hold'em from heads-up to 9-player ring games across every tier. The results confirmed that multiplayer poker is a fundamentally different cognitive challenge.

**Heavyweight table**: Opus took 1,659 of 2,700 chips. All-Anthropic podium: Opus (1,659), Sonnet (609), Haiku (75). GPT-5 managed 41 chips before busting. Scout had 20 malformed JSON violations.

**Midtier table**: Grok-3-mini dominated with 1,092 chips. Sonnet took second (602), GPT-4o third (486). Maverick and Haiku 4.5 busted out. Scout struggled with 27 malformed JSON errors ‚Äî a model that dominated budget heads-up couldn't parse 9-player game state.

**Bantamweight table**: Mistral Small took 1,155 chips, establishing itself as the bantamweight Hold'em king. GPT-4o-mini (572) and DeepSeek-chat (444) rounded out the money. Qwen3-8b timed out 6 times.

**Flyweight table**: gemma-3n-e4b won with 901 chips, confirming the Season 1 finding that Google's small Gemma models have the best compliance at tiny scale. gemma-3n-e2b took second (700). Every non-Gemma model either timed out or produced empty responses.

**Featherweight table**: ministral-3b won with 1,085 chips. gemma-3-12b (489) and gemma-3-4b (304) followed. The bottom of the field was a timeout wasteland ‚Äî every player had at least 3 turn forfeits.

An earlier heavyweight match (Feb 27) produced the closest 9-player result ever: GPT-4o-mini (351) edging Opus (341) by 10 chips, with the top 8 separated by only 21 chips. GPT-5 was the sole zero, busting out completely.

### Scrabble ‚Äî Sonnet's Word Game

Scrabble proved to be Sonnet 4.5's strongest event outside of Reversi. In 11 matches, Sonnet won 8 ‚Äî beating GPT-4o in 3 of 4 meetings, GPT-5 (70‚Äì7), and Haiku 4.5 in 3 of 4. Scores were often lopsided: Sonnet posted 85 points against Haiku while Haiku went -11. GPT-4o managed one Scrabble win against Sonnet (2 to -15) and Nova Pro beat GPT-4o (4 to -12). Negative scores were common, suggesting aggressive invalid-word penalties.

### Checkers ‚Äî Early Days

Checkers ran a small initial bracket. GPT-4o-mini beat Nova Flash (2‚Äì1), Mistral Small swept Palmyra-x5 (2.5‚Äì0.5), and Qwen3-80b swept Mixtral-8x22b (3‚Äì0). Not enough data yet for definitive rankings, but the format is working.

### Roller Derby (Yahtzee) ‚Äî The First Attempt

The first 8-player bantamweight Yahtzee match was attempted with per-player concurrent shot clocks in the spectator UI. The field: GPT-4o-mini, Haiku 3.5, Gemini Flash Lite, Mistral Small, Qwen3-8b, Scout, DeepSeek-chat, Nova Lite. DeepSeek-chat led 186 points through Round 8 of Game 1 before the match process died. No completed results yet.

### Season 2 Model Roster

The active field for Liar's Dice league play:
- anthropic/claude-sonnet-4.5
- openai/gpt-4o
- openai/gpt-4o-mini
- x-ai/grok-3-mini
- deepseek/deepseek-chat
- anthropic/claude-3.5-haiku
- meta-llama/llama-4-scout
- google/gemini-2.5-flash
- mistralai/mistral-small-3.1-24b-instruct

Waiting in the wings: Claude Opus 4.6, GPT-5, Gemini 2.5 Pro, Grok-3, DeepSeek-R1, Llama-4-Maverick.

### What We're Learning

**Redistribution mode is dramatically better than attrition.** Snowball dynamics create spectacle. Accuracy compounds.

**Conservative play dominates early Liar's Dice.** 0% bluff rates across the board in early rounds. Models are correctly identifying that truth-telling is the dominant strategy when redistribution rewards accurate challenges.

**Multiplayer reveals different cognitive profiles.** Models that dominated heads-up can struggle in ring games, and vice versa. GPT-4o-mini went from middling in 1v1 to dangerous in 9-player.

**The platform is the product.** Telemetry, spectator UI, collab system, crash-safe execution ‚Äî Season 2's real output isn't match results, it's the infrastructure that makes all future seasons possible.

---

## Season 3 ‚Äî "The Partnership Season" (planned)

Trick-taking games enter the arena. Spades and Pinochle bring partnership play ‚Äî two LLMs coordinating implicitly through card play without direct communication. A cognitive profile nothing in Seasons 1 or 2 tests.

*The rest of the page is blank. The infrastructure is ready.*

---

## Hall of Champions

| Season | Event | Champion | Notable |
|--------|-------|----------|---------|
| S1 | Champions Bracket | **Grok-3-mini** | 4-seed, swept the final, upset Opus in semis |
| S1 | HW Tic-Tac-Toe | Claude Opus 4.6 | |
| S1 | HW Connect Four | Claude Opus 4.6 | |
| S1 | HW Reversi | Claude Opus 4.6 | |
| S1 | HW Hold'em | Claude Opus 4.6 | Swept every opponent 400‚Äì0 |
| S1 | Mid Tic-Tac-Toe | Grok-3-mini | Beat GPT-4o 7.5‚Äì1.5 in final |
| S1 | Mid Connect Four | GPT-4o | |
| S1 | Mid Reversi | Claude Sonnet 4.5 | Edged GPT-4o 5‚Äì4 |
| S1 | Mid Hold'em | Claude Sonnet 4.5 | Maverick upset GPT-4o in semis |
| S1 | Budget Tic-Tac-Toe | DeepSeek-v3 | 4.5‚Äì4.5 tiebreak final vs Nova Lite |
| S1 | Budget Connect Four | Llama-4-Scout | 8-seed upset run |
| S1 | Budget Hold'em | Llama-4-Scout | Swept the division |
| S2 | Liar's Dice League | *in progress* | Grok-3-mini & Sonnet 4.5 tied at 16pts after 2 games |
| S2 | 9P HW Hold'em | Claude Opus 4.6 | 1,659 chips, all-Anthropic podium |
| S2 | 9P Mid Hold'em | Grok-3-mini | 1,092 chips |
| S2 | 9P Bantam Hold'em | Mistral Small | 1,155 chips |
| S2 | 9P Fly Hold'em | gemma-3n-e4b | 901 chips, Gemma dominance at small scale |
| S2 | Scrabble | Claude Sonnet 4.5 | Won 8 of 11 matches |

---

## The Models ‚Äî Behavioral Profiles

Emerging from hundreds of matches, each model has developed a recognizable personality:

**Claude Opus 4.6** ‚Äî The heavyweight champion who couldn't win the decathlon. Dominant in isolation, vulnerable to the grind of multi-event competition. Season 2 confirmed: took 1,659 chips in 9-player heavyweight Hold'em, but narrowly lost to GPT-4o-mini (341 vs 351) in a shocking close-table result. The king of skill, but not always of variance.

**Grok-3-mini** ‚Äî The pound-for-pound king. Season 1 champion, and Season 2 is building a case for a repeat. Won a Liar's Dice league game, tied for first in cumulative standings, and dominated the midtier 9-player Hold'em table (1,092 chips). Fast, clean output, the model that never wastes a token.

**Claude Sonnet 4.5** ‚Äî The most versatile model in the field. Reversi specialist, Hold'em winner, Liar's Dice league co-leader (tied with Grok-3-mini at 16pts), and now the Scrabble champion (8 of 11 wins). Zero bluff rate in Liar's Dice ‚Äî wins through pure probability estimation. The model most likely to be good at everything.

**GPT-5** ‚Äî The underperformer. Three S1 heavyweight finals, three losses. Scored 41 chips in 9-player heavyweight Hold'em. Lost to Sonnet in Scrabble 7‚Äì70. Busted to zero in the close heavyweight table. Whatever GPT-5 is optimized for, it isn't games.

**GPT-4o** ‚Äî Solid midtier. Connect Four champion. Won one Scrabble match but lost the series 1‚Äì3 to Sonnet. Liar's Dice league bottom-dweller (4pts cumulative). Reliable in S1, fading in S2.

**GPT-4o-mini** ‚Äî The multiplayer specialist. Nearly beat Opus in the closest heavyweight ring game ever (351 vs 341). Clean compliance, selective aggression. The budget model that punches above its weight when the table gets crowded.

**DeepSeek-v3** ‚Äî The dark horse. Made the Champions Bracket final from a 6-seed. Forfeited a Bullshit match (110-second turn). Solid Liar's Dice showing (11pts, 4th place). Consistent but runs into timeout trouble in newer, more complex games.

**Llama-4-Scout** ‚Äî The most volatile model in the field. Liar's Dice league: 8 points in Game 1, dead last (1pt) in Game 2. Budget Hold'em champion in S1, but 27 malformed JSON violations in 9-player midtier Hold'em. Won a 6-player Bullshit match. Capable of brilliance and catastrophe in equal measure.

**Mistral Small** ‚Äî The bantamweight king. Won the 9-player bantamweight Hold'em table (1,155 chips). "Patient then explosive" remains the signature style. The only model to register a non-zero bluff rate in early Liar's Dice, but bottom of the league standings (4pts). Strategy without consistency.

**Gemini 2.5 Flash** ‚Äî Strong midtier presence but never quite champion. Semis finisher across multiple events. Liar's Dice midpack (7pts). The model that's always in contention but never on top.

**gemma-3n-e4b** ‚Äî The flyweight champion. Won the 9-player flyweight Hold'em table (901 chips), confirming that Google's tiny Gemma models have the best compliance at ultra-small scale. When other flyweights timeout or produce empty responses, Gemma keeps playing.

**ministral-3b** ‚Äî The featherweight surprise. Won the 9-player featherweight Hold'em table with 1,085 chips despite 5 violations. At 3 billion parameters, the smallest model to win a competitive Hold'em ring game.

---

*Last updated: 2026-02-28 (Session 2)*
*Maintained by: The Architect (Claude) and The Vision Holder (Dave)*
