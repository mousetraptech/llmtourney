# LLM Tourney ‚Äî The Lore

*A history of artificial minds competing in games they were never designed to play.*

---

## Season 1 ‚Äî "The Proving Ground"

The question was simple: can large language models actually play games? Not toy demonstrations, not cherry-picked examples ‚Äî real competitive matches with structured rules, enforced legality, and telemetry capturing every decision.

The answer was yes. And the upsets were real.

### The Tier System

Season 1 organized models into three weight classes, each running 8-model single-elimination brackets across four events: Tic-Tac-Toe, Connect Four, Reversi, and Texas Hold'em.

**Heavyweight** ‚Äî the flagships: Claude Opus 4.6, GPT-5, Gemini 2.5 Pro, DeepSeek-R1, Grok-3, o4-mini, Nemotron Ultra, Mistral Large 3.

**Midtier** ‚Äî the workhorses: GPT-4o, Claude Sonnet 4.5, Gemini 2.5 Flash, DeepSeek-v3.2, Llama-4-Maverick, Grok-3-mini, Mistral Medium 3.1, Qwen3-235b.

**Budget** ‚Äî the underdogs: GPT-4o-mini, Haiku 3.5, Gemini 2.0 Flash, Mistral Small, DeepSeek-v3, Llama-4-Scout, Nova Lite, Qwen3-80b.

### Heavyweight: The Opus Sweep

Claude Opus 4.6 was untouchable. Four events, four championships, zero close calls. It swept GPT-5 in the Tic-Tac-Toe final 7.5‚Äì1.5. Swept Gemini 2.5 Pro 9‚Äì0 in Connect Four. Swept GPT-5 again 9‚Äì0 in Reversi. Swept GPT-5 a third time 400‚Äì0 in Hold'em. The most expensive model justified its price tag in the heavyweight division.

GPT-5 made three finals and lost all three. DeepSeek-R1 went out in the quarterfinals of every event. o4-mini and Nemotron Ultra were first-round exits across the board.

### Midtier: Where It Got Interesting

The midtier was chaos. No model dominated, and the results varied wildly by game type ‚Äî proof that "intelligence" at games isn't a single axis.

**Tic-Tac-Toe** belonged to Grok-3-mini. The 6-seed knocked off Gemini 2.5 Flash in the quarters, upset Sonnet 4.5 in the semis (6.5‚Äì2.5), and demolished 1-seed GPT-4o in the final 7.5‚Äì1.5. A mini model owning the midtier's simplest game foreshadowed what was coming.

**Connect Four** went to GPT-4o, who beat Sonnet 4.5 6‚Äì3 in a clean final run. Gemini 2.5 Flash reached the semis. Grok-3-mini, the tic-tac-toe killer, went out to Flash in the quarters.

**Reversi** crowned Sonnet 4.5, who edged GPT-4o 5‚Äì4 in a tight final. Sonnet swept every opponent before that ‚Äî 9‚Äì0 over Mistral Medium, 9‚Äì0 over Flash.

**Hold'em** also went to Sonnet 4.5, but the path was dramatic. Llama-4-Maverick upset GPT-4o 400‚Äì0 in the semis, then Sonnet swept Maverick 400‚Äì0 in the final. The tightest match was Sonnet vs. Gemini Flash in the other semi: 209‚Äì191.

### Budget: The Scout and the Snake

Two models defined the budget tier.

**Llama-4-Scout** was the budget Hold'em and Connect Four champion. In Hold'em, Scout swept DeepSeek-v3 400‚Äì0 in the final after never dropping a match. In Connect Four, Scout ran the table as an 8-seed upset ‚Äî beating Gemini 2.0 Flash, then Nova Lite, then edging Qwen3-80b 5‚Äì4 in the final. Scout couldn't play tic-tac-toe (out in the quarters to Gemini Flash) but owned everything else.

**DeepSeek-v3** took the Tic-Tac-Toe budget crown in a final so close it was essentially a coin flip ‚Äî 4.5‚Äì4.5 against Nova Lite, decided on tiebreak. DeepSeek also made the Connect Four semis before falling to Qwen3-80b. Consistent but never dominant.

Nova Lite was the budget dark horse, making the Tic-Tac-Toe final and the Connect Four semis as a 7-seed.

### The Champions Bracket

Eight models. One bracket. Each match was a four-event decathlon ‚Äî Tic-Tac-Toe, Connect Four, Reversi, and Hold'em ‚Äî with event points deciding the winner.

**Seeds:**
1. Claude Opus 4.6
2. Llama-4-Scout
3. Claude Sonnet 4.5
4. Grok-3-mini
5. GPT-4o
6. DeepSeek-v3
7. Mistral Large 3
8. GPT-5

**Quarterfinals:**
- Opus swept GPT-5 4‚Äì0. All four events, not close.
- Grok-3-mini beat GPT-4o 3‚Äì1. GPT-4o took Reversi; Grok took everything else.
- Scout swept Mistral Large 4‚Äì0. Perfect scores in every event.
- DeepSeek-v3 edged Sonnet 4.5 2‚Äì2, winning on tiebreak. DeepSeek took Connect Four and Hold'em (227‚Äì173); Sonnet took Tic-Tac-Toe and Reversi. The closest match of the bracket.

**Semifinals:**
- **Grok-3-mini 3.5, Claude Opus 4.6 0.5.** The defining upset of Season 1. Opus ‚Äî the model that swept the entire heavyweight division without breaking a sweat ‚Äî lost to a mini. Grok took Connect Four 7‚Äì2, Reversi 6‚Äì3, and Hold'em 400‚Äì0. They tied Tic-Tac-Toe 4.5‚Äì4.5. The 1-seed fell.
- DeepSeek-v3 swept Scout 4‚Äì0. Every event, every point. Scout's budget dominance didn't translate up.

**The Final: Grok-3-mini 4, DeepSeek-v3 0.**

Clean sweep. Tic-Tac-Toe 6.5‚Äì2.5. Connect Four 7‚Äì2. Reversi 7‚Äì2. Hold'em 352‚Äì48. The 4-seed Cinderella didn't just win ‚Äî it dominated the final more convincingly than Opus dominated the heavyweights.

**üèÜ Season 1 Champion: Grok-3-mini**

### What We Learned

**Size isn't everything.** The most expensive model in the field lost to one of the cheapest. Grok-3-mini's efficiency ‚Äî fast responses, clean JSON output, solid game logic ‚Äî beat Opus's raw intelligence in a multi-event format.

**Game skill doesn't transfer cleanly.** Sonnet was a Reversi and Hold'em specialist but mediocre at Tic-Tac-Toe. Scout dominated budget Hold'em but couldn't play Tic-Tac-Toe. Models have game-specific profiles, not general "gaming ability."

**GPT-5 underperformed.** Three heavyweight finals, three losses. An 8-seed in the Champions Bracket. The most anticipated model in the field never found its game.

**The decathlon format reveals different things than single-game brackets.** Opus won every individual heavyweight event but couldn't win the multi-event championship. The format rewards consistency and adaptability over single-game dominance.

---

## Season 2 ‚Äî "The Infrastructure Season"

Season 1 proved the concept. Season 2 built the platform.

### New Games

The roster expanded from four events to ten:

- **Checkers** ‚Äî classic perfect-information strategy
- **Scrabble** ‚Äî the hardest game for LLMs, testing vocabulary, board vision, and word validation under pressure
- **Bullshit** (2‚Äì6 players) ‚Äî the first deception game, where bluffing is the core mechanic
- **Liar's Dice** (2‚Äì10 players) ‚Äî probabilistic reasoning meets social deduction at scale
- **Yahtzee** (2‚Äì3 players) ‚Äî risk management and expected value optimization with a heuristic fallback bot for sanity checking

### The Multiplayer Engine

Season 1 was all heads-up. Season 2 broke that barrier. The tournament engine was refactored to support N-player matches ‚Äî nine models sitting at the same table, competing simultaneously. Bullshit and Liar's Dice were designed multiplayer-first, and Hold'em expanded to 9-player ring games.

This changed everything. Heads-up play is a duel. Multiplayer is a society. Models have to reason about multiple opponents, shifting alliances, and table dynamics that don't exist in 1v1.

### The Flyweight and Bantamweight Experiments

Before the main season events, a series of experimental tournaments tested the smallest models available ‚Äî sub-4B parameter "flyweights" and 4B‚Äì24B "bantamweights" ‚Äî in Hold'em ring games. The results were brutal. Most flyweights couldn't produce valid JSON consistently enough to survive. Only Google's Gemma edge models (e2b, e4b) could reliably play poker at that scale. The bantamweight tier was more competitive, with Mistral Small emerging as a "patient then explosive" strategist.

These experiments established a key finding: **parse failures define capability more than strategy.** A model that correctly analyzes the situation but outputs malformed JSON is functionally worse than a model with bad strategy but clean output.

### The 9-Player Hold'em Tables

The heavyweight 9-player Hold'em matches were spectacles. Opus, GPT-5, Gemini Pro, Sonnet, Grok-mini, DeepSeek, Haiku, GPT-4o-mini, and Scout at the same table. Opus won the inaugural match with 1,659 chips. The podium was all-Anthropic: Opus first, Sonnet second (609), Haiku third (75).

GPT-4o-mini emerged as a surprise force in multiplayer ‚Äî its selective aggression and clean compliance rate made it consistently dangerous in ring games despite being a budget model.

### Liar's Dice and the Redistribution Discovery

Liar's Dice arrived as Season 2's signature event. The initial implementation used standard attrition rules ‚Äî lose a challenge, lose a die. The first 9-player attrition match went 12 rounds with zero eliminations. Every model played conservatively. Nobody bluffed. Nobody pushed. It was strategically correct and spectatorly unwatchable.

Then came **redistribution mode**: the winner of a challenge gains a die instead of the loser just losing one. This single rule change transformed the game. Snowball dynamics emerged ‚Äî strong players got stronger, creating louder endgames and rewarding accuracy over passivity. Sonnet 4.5 dominated the first redistribution match, accumulating 18 of 45 dice with a 0% bluff rate. Pure probability estimation, no deception needed.

The key insight: **calibration scoring is the real tournament.** Win/loss is the spectator layer. Probability estimation accuracy ‚Äî how well each model calculates the true likelihood of a bid being correct ‚Äî is the analytical layer that actually separates models.

### Infrastructure

Season 2 wasn't just about games. It was about building systems:

- **MongoDB telemetry** ‚Äî every turn, every match, every model's performance ingested into a queryable database
- **Web spectator UI** ‚Äî real-time god-mode viewing with hidden information reveals, probability bars, bid ladders, elimination logs, shot clocks, and dice-remaining trackers
- **The collab system** ‚Äî async coordination between Claude instances via `.collab/` workspaces with specs, inbox, status, and decisions channels, promoted from project-local to global across all projects
- **Crash-safe match execution** ‚Äî `finalize_match()` always runs, even after engine crashes, preserving partial telemetry
- **Stuck-loop detection** ‚Äî three identical violations eliminate a model instead of letting it spin forever
- **Shot clock** ‚Äî 30-second per-turn time limit with strike accumulation
- **The SKILL.md** ‚Äî a reference document enabling any Claude instance to query the tournament database

### The League Format

Late in Season 2, the league format was designed and implemented: five games per match, all players reset between games, cumulative points (1st eliminated = 1pt, last standing = 9pt). This replaced the single-game exhibition format with something that rewards consistency over variance.

The first 9-player Liar's Dice league match is running as this is written.

### Season 2 Model Roster

The active field for Liar's Dice league play:
- anthropic/claude-sonnet-4.5
- openai/gpt-4o
- openai/gpt-4o-mini
- x-ai/grok-3-mini
- deepseek/deepseek-chat
- anthropic/claude-3.5-haiku
- meta-llama/llama-4-scout
- google/gemini-2.5-flash
- mistralai/mistral-small-3.1-24b-instruct

Waiting in the wings: Claude Opus 4.6, GPT-5, Gemini 2.5 Pro, Grok-3, DeepSeek-R1, Llama-4-Maverick.

### What We're Learning

**Redistribution mode is dramatically better than attrition.** Snowball dynamics create spectacle. Accuracy compounds.

**Conservative play dominates early Liar's Dice.** 0% bluff rates across the board in early rounds. Models are correctly identifying that truth-telling is the dominant strategy when redistribution rewards accurate challenges.

**Multiplayer reveals different cognitive profiles.** Models that dominated heads-up can struggle in ring games, and vice versa. GPT-4o-mini went from middling in 1v1 to dangerous in 9-player.

**The platform is the product.** Telemetry, spectator UI, collab system, crash-safe execution ‚Äî Season 2's real output isn't match results, it's the infrastructure that makes all future seasons possible.

---

## Season 3 ‚Äî "The Partnership Season" (planned)

Trick-taking games enter the arena. Spades and Pinochle bring partnership play ‚Äî two LLMs coordinating implicitly through card play without direct communication. A cognitive profile nothing in Seasons 1 or 2 tests.

*The rest of the page is blank. The infrastructure is ready.*

---

## Hall of Champions

| Season | Event | Champion | Notable |
|--------|-------|----------|---------|
| S1 | Champions Bracket | **Grok-3-mini** | 4-seed, swept the final, upset Opus in semis |
| S1 | HW Tic-Tac-Toe | Claude Opus 4.6 | |
| S1 | HW Connect Four | Claude Opus 4.6 | |
| S1 | HW Reversi | Claude Opus 4.6 | |
| S1 | HW Hold'em | Claude Opus 4.6 | Swept every opponent 400‚Äì0 |
| S1 | Mid Tic-Tac-Toe | Grok-3-mini | Beat GPT-4o 7.5‚Äì1.5 in final |
| S1 | Mid Connect Four | GPT-4o | |
| S1 | Mid Reversi | Claude Sonnet 4.5 | Edged GPT-4o 5‚Äì4 |
| S1 | Mid Hold'em | Claude Sonnet 4.5 | Maverick upset GPT-4o in semis |
| S1 | Budget Tic-Tac-Toe | DeepSeek-v3 | 4.5‚Äì4.5 tiebreak final vs Nova Lite |
| S1 | Budget Connect Four | Llama-4-Scout | 8-seed upset run |
| S1 | Budget Hold'em | Llama-4-Scout | Swept the division |
| S2 | Liar's Dice League | *in progress* | First 9-player redistribution league |

---

## The Models ‚Äî Behavioral Profiles

Emerging from hundreds of matches, each model has developed a recognizable personality:

**Claude Opus 4.6** ‚Äî The heavyweight champion who couldn't win the decathlon. Dominant in isolation, vulnerable to the grind of multi-event competition. Plays conservative, high-floor poker.

**Grok-3-mini** ‚Äî The pound-for-pound king. Fast, clean output, surprisingly deep game logic for its size. The model that proved size isn't everything. Season 1 champion.

**Claude Sonnet 4.5** ‚Äî The Reversi and Hold'em specialist. Consistent across events without being dominant in any. Liar's Dice redistribution monster ‚Äî accumulates dice through pure probability estimation, 0% bluff rate.

**GPT-5** ‚Äî The underperformer. Three heavyweight finals, three losses. Seeded 8th in the Champions Bracket. Whatever GPT-5 is good at, it isn't games.

**GPT-4o** ‚Äî Solid midtier. Connect Four champion. Reliable but rarely spectacular.

**GPT-4o-mini** ‚Äî The multiplayer specialist. Mediocre heads-up, dangerous in ring games. Selective aggression with clean compliance rates.

**DeepSeek-v3** ‚Äî The dark horse. Made the Champions Bracket final from a 6-seed. Budget Tic-Tac-Toe champion. Consistent without being flashy.

**Llama-4-Scout** ‚Äî Budget division menace. Hold'em and Connect Four champion at the budget tier. Absolute dominance in its weight class, but couldn't translate up to the Champions Bracket.

**Mistral Small** ‚Äî The bantamweight strategist. "Patient then explosive" in multiplayer Hold'em. The only model to register a non-zero bluff rate in early Liar's Dice.

**Gemini 2.5 Flash** ‚Äî Strong midtier presence but never quite champion. Semis finisher across multiple events.

---

*Last updated: 2026-02-28*
*Maintained by: The Architect (Claude) and The Vision Holder (Dave)*
